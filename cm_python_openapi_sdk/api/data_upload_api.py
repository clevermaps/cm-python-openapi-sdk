# coding: utf-8

"""
    clevermaps-client

    CleverMaps REST API provides access to location intelligence and geospatial analytics platform.  ### Main capabilities include: - Project and user access management - Multidimensional data model and metrics management - Data upload and exports - Ad-hoc analysis of geospatial data - Full text and geographic search - Configuration of CleverMaps Studio visualizations 

    The version of the OpenAPI document: 1.0.0
    Contact: support@clevermaps.io
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501

import warnings
from pydantic import validate_call, Field, StrictFloat, StrictStr, StrictInt
from typing import Any, Dict, List, Optional, Tuple, Union
from typing_extensions import Annotated

from pydantic import Field, StrictInt, StrictStr, field_validator
from typing import Optional
from typing_extensions import Annotated
from cm_python_openapi_sdk.models.data_complete_multipart_upload_request import DataCompleteMultipartUploadRequest
from cm_python_openapi_sdk.models.data_complete_multipart_upload_response import DataCompleteMultipartUploadResponse
from cm_python_openapi_sdk.models.data_upload200_response import DataUpload200Response

from cm_python_openapi_sdk.api_client import ApiClient, RequestSerialized
from cm_python_openapi_sdk.api_response import ApiResponse
from cm_python_openapi_sdk.rest import RESTResponseType


class DataUploadApi:
    """NOTE: This class is auto generated by OpenAPI Generator
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """

    def __init__(self, api_client=None) -> None:
        if api_client is None:
            api_client = ApiClient.get_default()
        self.api_client = api_client


    @validate_call
    def complete_multipart_upload(
        self,
        project_id: Annotated[str, Field(strict=True, description="Id of the project")],
        id: StrictStr,
        data_complete_multipart_upload_request: DataCompleteMultipartUploadRequest,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> DataCompleteMultipartUploadResponse:
        """Complete multipart upload

        This resource allows you to complete the multipart upload after all parts are uploaded. The request body contains the id received in the previous request (e.g. nogeLAvMPlITfWC66ztEDOW6Vl3bwRrn), the uploadId and a list of ETags for each part. ETag for each part can be found in the ETag response header from the PUT request to part upload.  **Security** Restricted to LOAD_DATA, DATA_EDITOR and ADMIN project's roles that have permission to load data into the project. 

        :param project_id: Id of the project (required)
        :type project_id: str
        :param id: (required)
        :type id: str
        :param data_complete_multipart_upload_request: (required)
        :type data_complete_multipart_upload_request: DataCompleteMultipartUploadRequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._complete_multipart_upload_serialize(
            project_id=project_id,
            id=id,
            data_complete_multipart_upload_request=data_complete_multipart_upload_request,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "DataCompleteMultipartUploadResponse",
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        ).data


    @validate_call
    def complete_multipart_upload_with_http_info(
        self,
        project_id: Annotated[str, Field(strict=True, description="Id of the project")],
        id: StrictStr,
        data_complete_multipart_upload_request: DataCompleteMultipartUploadRequest,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> ApiResponse[DataCompleteMultipartUploadResponse]:
        """Complete multipart upload

        This resource allows you to complete the multipart upload after all parts are uploaded. The request body contains the id received in the previous request (e.g. nogeLAvMPlITfWC66ztEDOW6Vl3bwRrn), the uploadId and a list of ETags for each part. ETag for each part can be found in the ETag response header from the PUT request to part upload.  **Security** Restricted to LOAD_DATA, DATA_EDITOR and ADMIN project's roles that have permission to load data into the project. 

        :param project_id: Id of the project (required)
        :type project_id: str
        :param id: (required)
        :type id: str
        :param data_complete_multipart_upload_request: (required)
        :type data_complete_multipart_upload_request: DataCompleteMultipartUploadRequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._complete_multipart_upload_serialize(
            project_id=project_id,
            id=id,
            data_complete_multipart_upload_request=data_complete_multipart_upload_request,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "DataCompleteMultipartUploadResponse",
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        )


    @validate_call
    def complete_multipart_upload_without_preload_content(
        self,
        project_id: Annotated[str, Field(strict=True, description="Id of the project")],
        id: StrictStr,
        data_complete_multipart_upload_request: DataCompleteMultipartUploadRequest,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> RESTResponseType:
        """Complete multipart upload

        This resource allows you to complete the multipart upload after all parts are uploaded. The request body contains the id received in the previous request (e.g. nogeLAvMPlITfWC66ztEDOW6Vl3bwRrn), the uploadId and a list of ETags for each part. ETag for each part can be found in the ETag response header from the PUT request to part upload.  **Security** Restricted to LOAD_DATA, DATA_EDITOR and ADMIN project's roles that have permission to load data into the project. 

        :param project_id: Id of the project (required)
        :type project_id: str
        :param id: (required)
        :type id: str
        :param data_complete_multipart_upload_request: (required)
        :type data_complete_multipart_upload_request: DataCompleteMultipartUploadRequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._complete_multipart_upload_serialize(
            project_id=project_id,
            id=id,
            data_complete_multipart_upload_request=data_complete_multipart_upload_request,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "DataCompleteMultipartUploadResponse",
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        return response_data.response


    def _complete_multipart_upload_serialize(
        self,
        project_id,
        id,
        data_complete_multipart_upload_request,
        _request_auth,
        _content_type,
        _headers,
        _host_index,
    ) -> RequestSerialized:

        _host = None

        _collection_formats: Dict[str, str] = {
        }

        _path_params: Dict[str, str] = {}
        _query_params: List[Tuple[str, str]] = []
        _header_params: Dict[str, Optional[str]] = _headers or {}
        _form_params: List[Tuple[str, str]] = []
        _files: Dict[
            str, Union[str, bytes, List[str], List[bytes], List[Tuple[str, bytes]]]
        ] = {}
        _body_params: Optional[bytes] = None

        # process the path parameters
        if project_id is not None:
            _path_params['projectId'] = project_id
        if id is not None:
            _path_params['id'] = id
        # process the query parameters
        # process the header parameters
        # process the form parameters
        # process the body parameter
        if data_complete_multipart_upload_request is not None:
            _body_params = data_complete_multipart_upload_request


        # set the HTTP header `Accept`
        if 'Accept' not in _header_params:
            _header_params['Accept'] = self.api_client.select_header_accept(
                [
                    'application/json'
                ]
            )

        # set the HTTP header `Content-Type`
        if _content_type:
            _header_params['Content-Type'] = _content_type
        else:
            _default_content_type = (
                self.api_client.select_header_content_type(
                    [
                        'application/json'
                    ]
                )
            )
            if _default_content_type is not None:
                _header_params['Content-Type'] = _default_content_type

        # authentication setting
        _auth_settings: List[str] = [
            'bearerAuth'
        ]

        return self.api_client.param_serialize(
            method='PUT',
            resource_path='/projects/{projectId}/dwh/data/uploads/{id}',
            path_params=_path_params,
            query_params=_query_params,
            header_params=_header_params,
            body=_body_params,
            post_params=_form_params,
            files=_files,
            auth_settings=_auth_settings,
            collection_formats=_collection_formats,
            _host=_host,
            _request_auth=_request_auth
        )




    @validate_call
    def data_upload(
        self,
        project_id: Annotated[str, Field(strict=True, description="Id of the project")],
        parts: Optional[StrictInt] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> DataUpload200Response:
        """Upload CSV file to dataset

        # Data Upload Methods  There are four ways to update dataset data:    1. **CSV Upload** – Push a CSV file from your computer or server.   2. **Multipart CSV Upload** – Push CSV file in parts from your computer or server (recommended for files larger then 50MB)     - you must specify parameter **parts**   3. **S3 Pull** – Pull data stored in your S3 bucket.   4. **HTTPS Pull** – Pull data from any HTTPS endpoint.  ---  ## CSV Upload Flow  1. Create a new upload resource via: `POST /rest/projects/{projectId}/dwh/data/uploads` 2. Receive a resource with an S3 pre-signed `uploadUrl`. 3. Upload the CSV file using: `PUT {uploadUrl}` (No authentication required) 4. Once the upload is finished, start a [dataPull](#operation/submitJobExecution) job with the correct input.  ### Pre-signed URL Usage The pre-signed URL can be used with any HTTP tool. You can upload the content directly without additional authorization (no bearer token required).  #### Example: Upload CSV File Using `curl` ```sh curl -X PUT \\ 'https://s3-eu-west-1.amazonaws.com/can-dwh-upload-uploads-prod/00ubftx6v6TSTKnAt0h7/o3d2ov0p43msl3hf/RjFJt5kW302QNLzFIVnxN369dlY6KUw4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJ6BXDNGJ7WFCT7SQ%2F20170915%2Feu-west-1%2Fs3%2Faws4_request&X-Amz-Date=20170915T141016Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e053197d044345041268b7f22918e2e71e2a3d5a23ceed65a332e1782998739c' \\ -T /home/user/data/mydata.csv ``` ## Multipart Upload Flow  To upload a large CSV file in multiple parts, follow these steps:  1. **Start the multipart upload** by sending a `POST` request: POST /rest/projects/{projectId}/dwh/data/uploads?parts=3   - This request initializes the upload and specifies the number of parts. 2. **Receive a response** containing:   - An `id` identifying the upload.   - A list of **uploadUrls**, which are S3 pre-signed URLs, one for each part. 3. **Split the CSV file**, compress each part with GZIP and upload each part to its corresponding URL using: PUT {uploadUrl}   - No additional authentication is required. 4. **Complete the upload**   - Once all parts are uploaded, send the [Complete Multipart Data Upload](#operation/completeMultipartUpload) request. 5. **Start the DataPull job**   - After completing the upload, start a [dataPull](#operation/submitJobExecution) job with the correct input.  ## S3 Pull Flow  Start a [dataPull](#operation/submitJobExecution) job using an **S3 URL**.  ### Example: DataPull Request for S3 Upload ```json {   \"type\": \"dataPull\",   \"projectId\": \"ncesksvg7rjeri7v\",   \"content\": {     \"dataset\": \"mystores\",     \"mode\": \"full\",     \"s3Upload\": {       \"uri\": \"s3://can-s3-dwh-pull-test/mystores.csv\",       \"accessKeyId\": \"myAccessKey\",       \"secretAccessKey\": \"mySecretAccessKey\"     },     \"type\": \"csv\"   } } ``` ## HTTPS Pull Flow  Start a [dataPull](#operation/submitJobExecution) job using an **HTTPS URL**.  ### Example: DataPull Request for HTTPS Upload ```json {   \"type\": \"dataPull\",   \"projectId\": \"ncesksvg7rjeri7v\",   \"content\": {     \"dataset\": \"mystores\",     \"mode\": \"full\",     \"httpsUpload\": \"https://my-domain.com/mystores.csv\",     \"type\": \"csv\"   } } ``` **Warning**: We do not recommend uploading sensitive information using the HTTPS pull flow, as files hosted on public URLs are accessible to anyone on the internet.  **Security** Restricted to LOAD_DATA, DATA_EDITOR and ADMIN project's roles that have permission to load data into the project.  ## CSV Options  With **CsvOptions**, you can specify how the CSV should be processed. The available options are:  | Option       | Type      | Description                                                   | Default Value | |-------------|----------|---------------------------------------------------------------|--------------| | `header`    | boolean  | Specifies if the CSV has a header.                           | `true`       | | `separator` | char     | Specifies the separator character between fields.           | `,`          | | `quote`     | char     | Specifies the quote character.                              | `\"`          | | `escape`    | char     | Specifies the escape character.                             | `\\`          | | `null`      | string   | Specifies the replacement value for custom CSV nulls.      | *None*       | | `forceNull` | string[] | Specifies which CSV columns should enforce null replacement. | *None*       |  ### Example: DataPull Request for S3 Upload with CSV Options ```json {   \"type\": \"dataPull\",   \"projectId\": \"ncesksvg7rjeri7v\",   \"content\": {     \"dataset\": \"mystores\",     \"mode\": \"full\",     \"s3Upload\": {       \"uri\": \"s3://can-s3-dwh-pull-test/mystores.csv\",       \"accessKeyId\": \"myAccessKey\",       \"secretAccessKey\": \"mySecretAccessKey\"     },     \"type\": \"csv\",     \"csvOptions\": {       \"header\": true,       \"separator\": \";\",       \"quote\": \"§\",       \"escape\": \"\\\\\",       \"null\": \"NULL_VALUE\",       \"forceNull\": [         \"columnName\",         \"otherColumn\"       ]     }   } } ``` 

        :param project_id: Id of the project (required)
        :type project_id: str
        :param parts:
        :type parts: int
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._data_upload_serialize(
            project_id=project_id,
            parts=parts,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "DataUpload200Response",
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        ).data


    @validate_call
    def data_upload_with_http_info(
        self,
        project_id: Annotated[str, Field(strict=True, description="Id of the project")],
        parts: Optional[StrictInt] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> ApiResponse[DataUpload200Response]:
        """Upload CSV file to dataset

        # Data Upload Methods  There are four ways to update dataset data:    1. **CSV Upload** – Push a CSV file from your computer or server.   2. **Multipart CSV Upload** – Push CSV file in parts from your computer or server (recommended for files larger then 50MB)     - you must specify parameter **parts**   3. **S3 Pull** – Pull data stored in your S3 bucket.   4. **HTTPS Pull** – Pull data from any HTTPS endpoint.  ---  ## CSV Upload Flow  1. Create a new upload resource via: `POST /rest/projects/{projectId}/dwh/data/uploads` 2. Receive a resource with an S3 pre-signed `uploadUrl`. 3. Upload the CSV file using: `PUT {uploadUrl}` (No authentication required) 4. Once the upload is finished, start a [dataPull](#operation/submitJobExecution) job with the correct input.  ### Pre-signed URL Usage The pre-signed URL can be used with any HTTP tool. You can upload the content directly without additional authorization (no bearer token required).  #### Example: Upload CSV File Using `curl` ```sh curl -X PUT \\ 'https://s3-eu-west-1.amazonaws.com/can-dwh-upload-uploads-prod/00ubftx6v6TSTKnAt0h7/o3d2ov0p43msl3hf/RjFJt5kW302QNLzFIVnxN369dlY6KUw4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJ6BXDNGJ7WFCT7SQ%2F20170915%2Feu-west-1%2Fs3%2Faws4_request&X-Amz-Date=20170915T141016Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e053197d044345041268b7f22918e2e71e2a3d5a23ceed65a332e1782998739c' \\ -T /home/user/data/mydata.csv ``` ## Multipart Upload Flow  To upload a large CSV file in multiple parts, follow these steps:  1. **Start the multipart upload** by sending a `POST` request: POST /rest/projects/{projectId}/dwh/data/uploads?parts=3   - This request initializes the upload and specifies the number of parts. 2. **Receive a response** containing:   - An `id` identifying the upload.   - A list of **uploadUrls**, which are S3 pre-signed URLs, one for each part. 3. **Split the CSV file**, compress each part with GZIP and upload each part to its corresponding URL using: PUT {uploadUrl}   - No additional authentication is required. 4. **Complete the upload**   - Once all parts are uploaded, send the [Complete Multipart Data Upload](#operation/completeMultipartUpload) request. 5. **Start the DataPull job**   - After completing the upload, start a [dataPull](#operation/submitJobExecution) job with the correct input.  ## S3 Pull Flow  Start a [dataPull](#operation/submitJobExecution) job using an **S3 URL**.  ### Example: DataPull Request for S3 Upload ```json {   \"type\": \"dataPull\",   \"projectId\": \"ncesksvg7rjeri7v\",   \"content\": {     \"dataset\": \"mystores\",     \"mode\": \"full\",     \"s3Upload\": {       \"uri\": \"s3://can-s3-dwh-pull-test/mystores.csv\",       \"accessKeyId\": \"myAccessKey\",       \"secretAccessKey\": \"mySecretAccessKey\"     },     \"type\": \"csv\"   } } ``` ## HTTPS Pull Flow  Start a [dataPull](#operation/submitJobExecution) job using an **HTTPS URL**.  ### Example: DataPull Request for HTTPS Upload ```json {   \"type\": \"dataPull\",   \"projectId\": \"ncesksvg7rjeri7v\",   \"content\": {     \"dataset\": \"mystores\",     \"mode\": \"full\",     \"httpsUpload\": \"https://my-domain.com/mystores.csv\",     \"type\": \"csv\"   } } ``` **Warning**: We do not recommend uploading sensitive information using the HTTPS pull flow, as files hosted on public URLs are accessible to anyone on the internet.  **Security** Restricted to LOAD_DATA, DATA_EDITOR and ADMIN project's roles that have permission to load data into the project.  ## CSV Options  With **CsvOptions**, you can specify how the CSV should be processed. The available options are:  | Option       | Type      | Description                                                   | Default Value | |-------------|----------|---------------------------------------------------------------|--------------| | `header`    | boolean  | Specifies if the CSV has a header.                           | `true`       | | `separator` | char     | Specifies the separator character between fields.           | `,`          | | `quote`     | char     | Specifies the quote character.                              | `\"`          | | `escape`    | char     | Specifies the escape character.                             | `\\`          | | `null`      | string   | Specifies the replacement value for custom CSV nulls.      | *None*       | | `forceNull` | string[] | Specifies which CSV columns should enforce null replacement. | *None*       |  ### Example: DataPull Request for S3 Upload with CSV Options ```json {   \"type\": \"dataPull\",   \"projectId\": \"ncesksvg7rjeri7v\",   \"content\": {     \"dataset\": \"mystores\",     \"mode\": \"full\",     \"s3Upload\": {       \"uri\": \"s3://can-s3-dwh-pull-test/mystores.csv\",       \"accessKeyId\": \"myAccessKey\",       \"secretAccessKey\": \"mySecretAccessKey\"     },     \"type\": \"csv\",     \"csvOptions\": {       \"header\": true,       \"separator\": \";\",       \"quote\": \"§\",       \"escape\": \"\\\\\",       \"null\": \"NULL_VALUE\",       \"forceNull\": [         \"columnName\",         \"otherColumn\"       ]     }   } } ``` 

        :param project_id: Id of the project (required)
        :type project_id: str
        :param parts:
        :type parts: int
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._data_upload_serialize(
            project_id=project_id,
            parts=parts,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "DataUpload200Response",
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        )


    @validate_call
    def data_upload_without_preload_content(
        self,
        project_id: Annotated[str, Field(strict=True, description="Id of the project")],
        parts: Optional[StrictInt] = None,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> RESTResponseType:
        """Upload CSV file to dataset

        # Data Upload Methods  There are four ways to update dataset data:    1. **CSV Upload** – Push a CSV file from your computer or server.   2. **Multipart CSV Upload** – Push CSV file in parts from your computer or server (recommended for files larger then 50MB)     - you must specify parameter **parts**   3. **S3 Pull** – Pull data stored in your S3 bucket.   4. **HTTPS Pull** – Pull data from any HTTPS endpoint.  ---  ## CSV Upload Flow  1. Create a new upload resource via: `POST /rest/projects/{projectId}/dwh/data/uploads` 2. Receive a resource with an S3 pre-signed `uploadUrl`. 3. Upload the CSV file using: `PUT {uploadUrl}` (No authentication required) 4. Once the upload is finished, start a [dataPull](#operation/submitJobExecution) job with the correct input.  ### Pre-signed URL Usage The pre-signed URL can be used with any HTTP tool. You can upload the content directly without additional authorization (no bearer token required).  #### Example: Upload CSV File Using `curl` ```sh curl -X PUT \\ 'https://s3-eu-west-1.amazonaws.com/can-dwh-upload-uploads-prod/00ubftx6v6TSTKnAt0h7/o3d2ov0p43msl3hf/RjFJt5kW302QNLzFIVnxN369dlY6KUw4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJ6BXDNGJ7WFCT7SQ%2F20170915%2Feu-west-1%2Fs3%2Faws4_request&X-Amz-Date=20170915T141016Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e053197d044345041268b7f22918e2e71e2a3d5a23ceed65a332e1782998739c' \\ -T /home/user/data/mydata.csv ``` ## Multipart Upload Flow  To upload a large CSV file in multiple parts, follow these steps:  1. **Start the multipart upload** by sending a `POST` request: POST /rest/projects/{projectId}/dwh/data/uploads?parts=3   - This request initializes the upload and specifies the number of parts. 2. **Receive a response** containing:   - An `id` identifying the upload.   - A list of **uploadUrls**, which are S3 pre-signed URLs, one for each part. 3. **Split the CSV file**, compress each part with GZIP and upload each part to its corresponding URL using: PUT {uploadUrl}   - No additional authentication is required. 4. **Complete the upload**   - Once all parts are uploaded, send the [Complete Multipart Data Upload](#operation/completeMultipartUpload) request. 5. **Start the DataPull job**   - After completing the upload, start a [dataPull](#operation/submitJobExecution) job with the correct input.  ## S3 Pull Flow  Start a [dataPull](#operation/submitJobExecution) job using an **S3 URL**.  ### Example: DataPull Request for S3 Upload ```json {   \"type\": \"dataPull\",   \"projectId\": \"ncesksvg7rjeri7v\",   \"content\": {     \"dataset\": \"mystores\",     \"mode\": \"full\",     \"s3Upload\": {       \"uri\": \"s3://can-s3-dwh-pull-test/mystores.csv\",       \"accessKeyId\": \"myAccessKey\",       \"secretAccessKey\": \"mySecretAccessKey\"     },     \"type\": \"csv\"   } } ``` ## HTTPS Pull Flow  Start a [dataPull](#operation/submitJobExecution) job using an **HTTPS URL**.  ### Example: DataPull Request for HTTPS Upload ```json {   \"type\": \"dataPull\",   \"projectId\": \"ncesksvg7rjeri7v\",   \"content\": {     \"dataset\": \"mystores\",     \"mode\": \"full\",     \"httpsUpload\": \"https://my-domain.com/mystores.csv\",     \"type\": \"csv\"   } } ``` **Warning**: We do not recommend uploading sensitive information using the HTTPS pull flow, as files hosted on public URLs are accessible to anyone on the internet.  **Security** Restricted to LOAD_DATA, DATA_EDITOR and ADMIN project's roles that have permission to load data into the project.  ## CSV Options  With **CsvOptions**, you can specify how the CSV should be processed. The available options are:  | Option       | Type      | Description                                                   | Default Value | |-------------|----------|---------------------------------------------------------------|--------------| | `header`    | boolean  | Specifies if the CSV has a header.                           | `true`       | | `separator` | char     | Specifies the separator character between fields.           | `,`          | | `quote`     | char     | Specifies the quote character.                              | `\"`          | | `escape`    | char     | Specifies the escape character.                             | `\\`          | | `null`      | string   | Specifies the replacement value for custom CSV nulls.      | *None*       | | `forceNull` | string[] | Specifies which CSV columns should enforce null replacement. | *None*       |  ### Example: DataPull Request for S3 Upload with CSV Options ```json {   \"type\": \"dataPull\",   \"projectId\": \"ncesksvg7rjeri7v\",   \"content\": {     \"dataset\": \"mystores\",     \"mode\": \"full\",     \"s3Upload\": {       \"uri\": \"s3://can-s3-dwh-pull-test/mystores.csv\",       \"accessKeyId\": \"myAccessKey\",       \"secretAccessKey\": \"mySecretAccessKey\"     },     \"type\": \"csv\",     \"csvOptions\": {       \"header\": true,       \"separator\": \";\",       \"quote\": \"§\",       \"escape\": \"\\\\\",       \"null\": \"NULL_VALUE\",       \"forceNull\": [         \"columnName\",         \"otherColumn\"       ]     }   } } ``` 

        :param project_id: Id of the project (required)
        :type project_id: str
        :param parts:
        :type parts: int
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._data_upload_serialize(
            project_id=project_id,
            parts=parts,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "DataUpload200Response",
        }
        response_data = self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        return response_data.response


    def _data_upload_serialize(
        self,
        project_id,
        parts,
        _request_auth,
        _content_type,
        _headers,
        _host_index,
    ) -> RequestSerialized:

        _host = None

        _collection_formats: Dict[str, str] = {
        }

        _path_params: Dict[str, str] = {}
        _query_params: List[Tuple[str, str]] = []
        _header_params: Dict[str, Optional[str]] = _headers or {}
        _form_params: List[Tuple[str, str]] = []
        _files: Dict[
            str, Union[str, bytes, List[str], List[bytes], List[Tuple[str, bytes]]]
        ] = {}
        _body_params: Optional[bytes] = None

        # process the path parameters
        if project_id is not None:
            _path_params['projectId'] = project_id
        # process the query parameters
        if parts is not None:
            
            _query_params.append(('parts', parts))
            
        # process the header parameters
        # process the form parameters
        # process the body parameter


        # set the HTTP header `Accept`
        if 'Accept' not in _header_params:
            _header_params['Accept'] = self.api_client.select_header_accept(
                [
                    'application/json'
                ]
            )


        # authentication setting
        _auth_settings: List[str] = [
            'bearerAuth'
        ]

        return self.api_client.param_serialize(
            method='POST',
            resource_path='/projects/{projectId}/dwh/data/uploads',
            path_params=_path_params,
            query_params=_query_params,
            header_params=_header_params,
            body=_body_params,
            post_params=_form_params,
            files=_files,
            auth_settings=_auth_settings,
            collection_formats=_collection_formats,
            _host=_host,
            _request_auth=_request_auth
        )


